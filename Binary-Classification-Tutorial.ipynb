{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e784baa7-4fab-459d-a282-952dc5ab6c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature name ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "feature target ['setosa' 'versicolor' 'virginica']\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# This is the first stage of any ML project — loading the data.\n",
    "# It can come from CSV, Excel, JSON, database, or an API.\n",
    "# For now, we're using a built-in dataset just to keep things simple.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "print(x[:5])\n",
    "\n",
    "# As you can see in the output, each row represents a plant species with its different features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918c0683-d03f-4ffe-b974-5454774bf585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-train shape (90, 4)\n",
      "x-test shape (60, 4)\n",
      "y-train shape (90,)\n",
      "y-test shape (60,)\n"
     ]
    }
   ],
   "source": [
    "# Now we split the data into training and testing sets.\n",
    "# We've set 40% of the data for testing, so the remaining 60% is used for training.\n",
    "# If you're unsure what ratio to use, techniques like cross-validation can help decide.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=1)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# In the output, you'll see:\n",
    "# - x_train has 90 rows and 4 columns. This means we're using 90 samples (each with 4 features) for training.\n",
    "# - x_test has 60 rows and 4 columns — so 60 samples (each with 4 features) for testing.\n",
    "# - y_train has 90 rows with just one column. These are the actual labels or targets the model should learn from.\n",
    "# - y_test has 60 rows and one column — the actual target values the model will be tested on.\n",
    "# Since `y` represents the output (or target), it doesn't need any features — it’s simply one value per sample, like a price, a category, or a score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcbd2754-2d4b-4c5c-b56b-1429bbff6a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 📌 Binary Classification Tutorial — Building & Understanding Models\n",
    "# ----------------------------------------------------------\n",
    "# This section shows how to train a simple binary classifier using Logistic Regression.\n",
    "# Later, we’ll go over various models, when to use each, and real-world scenarios.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Training the Model\n",
    "# -------------------\n",
    "# Logistic Regression is a commonly used algorithm for binary classification problems.\n",
    "# For example: spam detection, disease prediction, etc.\n",
    "# It assumes a linear relationship between features and the log-odds of the output.\n",
    "\n",
    "model = LogisticRegression()                # Step 1: Create the logistic regression model\n",
    "model.fit(x_train, y_train)                 # Step 2: Train the model using training data\n",
    "\n",
    "# Testing the Model\n",
    "# -------------------\n",
    "y_pred = model.predict(x_test)              # Step 3: Predict the labels for the test data\n",
    "\n",
    "# Evaluating the Model\n",
    "# ---------------------\n",
    "accuracy = accuracy_score(y_test, y_pred)   # Step 4: Compare predicted vs actual test labels\n",
    "print(\"The accuracy:\", accuracy)\n",
    "\n",
    "# 🎯 If accuracy is ~0.96 (or 96%), it means that 96% of test samples were correctly classified.\n",
    "# That’s a strong result for a simple binary classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b121b9-4f3e-45f1-b30b-91cc64e982c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Choosing the Right Machine Learning Algorithm\n",
    "# Each algorithm has a core intuition — a \"way of thinking\" about data.\n",
    "# This guide captures the essence of each method and when it naturally fits.\n",
    "\n",
    "# 🧠 Logistic Regression\n",
    "# Essence: Finds the simplest boundary (a line or hyperplane) that separates classes.\n",
    "# Best For: Linearly separable classes where speed and interpretability matter.\n",
    "# Intuition: If the weighted sum of features crosses a threshold, it's class 1; else class 0.\n",
    "\n",
    "# 🦉 Naive Bayes\n",
    "# Essence: Uses probabilities assuming features are independent.\n",
    "# Best For: Text classification like spam detection or sentiment analysis.\n",
    "# Intuition: Multiply the probability of each feature given a class, pick the class with the highest product.\n",
    "\n",
    "# 🌳 Decision Tree\n",
    "# Essence: Asks a sequence of yes/no questions to split data based on most informative features.\n",
    "# Best For: Problems with clear logical rules or non-linear relationships.\n",
    "# Intuition: Recursively split the data using the best feature until pure or max depth.\n",
    "\n",
    "# 🌲 Random Forest\n",
    "# Essence: Combines many decision trees trained on different parts of the data and features.\n",
    "# Best For: Tabular data with noise or potential overfitting.\n",
    "# Intuition: Let many weak models vote; their average is more stable than one.\n",
    "\n",
    "# 🌀 Support Vector Machine (SVM)\n",
    "# Essence: Finds the widest possible margin between classes using only critical support vectors.\n",
    "# Best For: High-dimensional data with clear margins — text, bioinformatics, etc.\n",
    "# Intuition: Find the cleanest line (or hyperplane) that separates the classes with maximum margin.\n",
    "\n",
    "# 👟 K-Nearest Neighbors (KNN)\n",
    "# Essence: Classifies a point based on the majority label of its nearest neighbors.\n",
    "# Best For: Small datasets or low-dimensional spaces where similarity matters.\n",
    "# Intuition: No training. Just store the data and look around when a new point comes.\n",
    "\n",
    "# 🧠 Neural Networks\n",
    "# Essence: Learns layered representations of input by passing data through multiple transformations.\n",
    "# Best For: Complex tasks like image recognition, speech, or language modeling on large datasets.\n",
    "# Intuition: Each layer extracts higher-level features; like how brains learn abstract patterns.\n",
    "\n",
    "# 💡 Tip:\n",
    "# Start simple (like Logistic Regression or Naive Bayes).\n",
    "# Move to more complex models (SVM, Random Forest, Neural Nets) only if performance demands it.\n",
    "# Always validate using test data to avoid overfitting and ensure generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fc39852-0b48-47d0-9d3e-f87e5919173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# 🧠 Note:\n",
    "# Although we have already trained and tested the model,\n",
    "# here we are revisiting an earlier pipeline step (encoding) \n",
    "# to better understand how categorical data is converted into numeric form.\n",
    "# This explanation is for learning purposes.\n",
    "\n",
    "# 🔁 This is the second stage of the pipeline — encoding.\n",
    "# In this step, we convert categorical data (like 'dog', 'cat', 'bird') into numerical form.\n",
    "# LabelEncoder assigns numeric values based on label order, like: bird=0, cat=1, dog=2.\n",
    "# But this imposes an artificial priority (dog > cat > bird), which may not make sense for many models.\n",
    "# In such cases, we prefer OneHotEncoding instead — which we'll see after this.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_data = ['dog', 'cat', 'cat', 'dog', 'bird']\n",
    "encoder = LabelEncoder()\n",
    "encoded = encoder.fit_transform(categorical_data)\n",
    "print(encoded)  # Output: [2 1 1 2 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d522e58c-bea2-4cd5-983b-9ea1b3edb41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded features:\n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 🧠 Continuing from the previous step:\n",
    "# LabelEncoder gave us integer values, but those integers implied an order (e.g., dog > cat > bird),\n",
    "# which isn't appropriate for most machine learning models.\n",
    "# So instead, we use OneHotEncoder to represent categories *without* implying any order.\n",
    "\n",
    "# 📌 OneHotEncoder creates a binary column for each category:\n",
    "# For example:\n",
    "# 'dog'  → [0. 0. 1.]\n",
    "# 'cat'  → [0. 1. 0.]\n",
    "# 'bird' → [1. 0. 0.]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Input categorical data\n",
    "categorical_data = ['dog', 'cat', 'cat', 'dog', 'bird']\n",
    "\n",
    "# Reshape the data into a 2D array as required by OneHotEncoder\n",
    "categorical_data = np.array(categorical_data).reshape(-1, 1)\n",
    "\n",
    "# Initialize the encoder (set sparse_output=False to get dense NumPy array)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit the encoder and transform the data\n",
    "encoded_features = encoder.fit_transform(categorical_data)\n",
    "\n",
    "# Print the resulting one-hot encoded matrix\n",
    "print(\"One-hot encoded features:\\n\", encoded_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1358f227-d7bb-4f5b-bcca-206e75168d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 📌 Conclusion\n",
    "\n",
    "#In this notebook, we built a simple binary classification pipeline using Logistic Regression. We covered essential preprocessing steps like label encoding, one-hot encoding, and model evaluation using accuracy.\n",
    "\n",
    "#Thanks for reading! 😊  \n",
    "#Feel free to ⭐️ the repo or leave feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2715d842-fabe-4248-b781-acb1f1601e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\bowjo'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d126e4a-b3de-45fe-90c1-6ab449aede88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dir>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b46e62-a20d-49e2-8d7d-7cad0d2da794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
